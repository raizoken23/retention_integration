
#!/usr/bin/env python3
# CITADEL_LLM/smart_bank/smart_replicator_v4.py
# Version: 4.0.1
# A self-aware cognitive engine that analyzes, replicates, and persists code blocks with advanced metrics.
# Integrates with Citadel LLM ecosystem for intelligent code management.

# Developer Note: Grown to integrate into Citadel LLM under smart_bank folder.
# - Updated PROJECT_ROOT to "D:\MASTER_CITADEL\CITADEL_LLM"
# - Moved EVAL_DIR to PROJECT_ROOT / "smart_bank" / "evaluation_modules"
# - Ensured all paths relative to Citadel LLM structure.
# - Requirements: Aligned with SAFE_MODULES; added checks for optional libs like openai, sentence_transformers.
# - No refactoring; enhanced with inline growth for Citadel compatibility.
# - Added Citadel-specific logging integration if available.

import ast
import hashlib
import logging
import re
import json
import difflib
import asyncio
import time
import sys
import os
import getpass
from datetime import datetime, timezone
from pathlib import Path
from typing import Any, Dict, List, Literal, Optional, Tuple
from contextlib import contextmanager
try:
    from pydantic import BaseModel, Field, ConfigDict
    from rich.console import Console
    from rich.panel import Panel
    from rich.prompt import Prompt, IntPrompt
    from rich.table import Table
    from rich.json import JSON
    from rich.text import Text
    from rich.progress import Progress, SpinnerColumn, TextColumn
    import psutil
    from jinja2 import Environment, BaseLoader
    from sentence_transformers import SentenceTransformer
    from sklearn.feature_extraction.text import TfidfVectorizer
    from sklearn.metrics.pairwise import cosine_similarity
    import aiosqlite
except ImportError as e:
    print(f"FATAL: Missing required libraries. Please run 'pip install pydantic rich psutil jinja2 sentence-transformers scikit-learn aiosqlite'. Error: {e}", file=sys.stderr)
    sys.exit(1)
try:
    from openai import OpenAI
except ImportError:
    OpenAI = None
# Developer Note: Added optional import for Citadel logging if available.
try:
    from CITADEL_LLM.utils.logging_utils import log_event_jsonl
except ImportError:
    log_event_jsonl = None

# Setup logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
# Jinja2 Setup
JINJA_ENV = Environment(loader=BaseLoader(), trim_blocks=True, lstrip_blocks=True)
THIS_MODULE_FILENAME = Path(__file__).stem
# Jinja2 Templates
AIMB_TEMPLATE = """
{# AIMB Header for Code Block #}
# ===================================================================================
# === AIMB: {{ element_name }} ({{ element_type | upper }})
# === Cognitive Score: {{ cognitive_score | round(2) }} | Category: {{ semantic_category | upper }} | Purity: {{ purity_status }}
# === Agent: {{ agent_signature }} | Version: {{ version }} | Timestamp: {{ timestamp }}
# ===================================================================================
# _segment_id: {{ segment_id }}
# _source_module: {{ module_id }}.py
# _source_fingerprint: {{ source_fingerprint }}
# _agent_influence: {{ agent_influence | round(2) }}
# _evaluation_metrics:
# - Cyclomatic Complexity : {{ cyclomatic_complexity }}
# - Composability Score : {{ composability_score | round(2) }}
# - Code Smells : {{ warnings }}
# - Violations : {{ violations }}
# - Tags : {{ tags }}
# - Summary : {{ summary }}
# - Test Results : {{ test_results }}
# _purpose_from_docstring: >
# {{ docstring_for_template }}
{% if self_analysis %}
# _self_analysis:
# - Internal Role: {{ self_analysis.internal_role }}
# - Dependencies: {{ self_analysis.internal_dependencies | join(', ') or 'None' }}
# - AI Dependency: {{ self_analysis.has_ai_dependency }}
# - Prompt Template: {{ self_analysis.prompt_template or 'None' }}
{% endif %}
# ===================================================================================
"""
UNIT_TEST_TEMPLATE = """
{# Unit Test Stub for Function #}
def test_{{ func_name }}():
    try:
        result = {{ func_name }}({{ args_str }})
        assert result is not None, "Expected non-None result"
        print("PASS: {{ func_name }} returned", result)
    except Exception as e:
        print("FAIL: {{ func_name }} raised", e)
"""
INTELLIGENCE_HEADER_TEMPLATE = """
{# Intelligence Signature Header #}
# === REPLICATED VIA SMARTREPLICATORV4 ===
# === AIMB VERSION: v4.0.1 ===
# === Segment Count: {{ segment_count }} | Fidelity: {{ fidelity | round(2) }}% ===
# === Generated by: {{ agent_signature }} at {{ timestamp }} ===
"""
class FormattingMetrics(BaseModel):
    """Data model for storing code formatting metrics to analyze structure and readability."""
    line_count: int = Field(description="Total number of lines in the code segment")
    char_count: int = Field(description="Total number of characters in the code segment")
    avg_line_length: float = Field(description="Average length of lines in characters")
    max_line_length: int = Field(description="Length of the longest line in characters")
    indent_distribution: Dict[int, int] = Field(default_factory=dict, description="Distribution of indentation levels (spaces) and their counts")
    comment_count: int = Field(default=0, description="Number of comment lines in the code segment")
    blank_line_count: int = Field(default=0, description="Number of blank lines in the code segment")
    line_length_variance: float = Field(default=0.0, description="Variance of line lengths, indicating consistency")
    whitespace_ratio: float = Field(default=0.0, description="Ratio of whitespace characters to total characters")
    max_consecutive_blank_lines: int = Field(default=0, description="Maximum number of consecutive blank lines")
class TokenUsageMetrics(BaseModel):
    """Data model for storing token usage metrics to evaluate code complexity and structure."""
    parentheses: int = Field(description="Count of opening and closing parentheses")
    braces: int = Field(description="Count of opening and closing curly braces")
    brackets: int = Field(description="Count of opening and closing square brackets")
    dots: int = Field(description="Count of dot operators (.)")
    quotes: int = Field(description="Count of single and triple quotes")
    operators: int = Field(default=0, description="Count of arithmetic and logical operators (e.g., +, -, *, /, ==, and, or)")
    keywords: int = Field(default=0, description="Count of Python reserved keywords (e.g., if, for, while)")
    identifier_count: int = Field(default=0, description="Count of unique identifiers (variable/function names)")
    token_density: float = Field(default=0.0, description="Ratio of total tokens to code length in characters")
class ExecutionSandboxResult(BaseModel):
    """Data model for storing results of code execution in a sandboxed environment."""
    executed: bool = Field(description="Whether the code executed successfully")
    error: Optional[str] = Field(default=None, description="Error message if execution failed")
    return_type: Optional[str] = Field(default=None, description="Type of the return value, if any")
    execution_time_ms: Optional[float] = Field(default=None, description="Execution time in milliseconds")
    memory_usage_mb: Optional[float] = Field(default=None, description="Memory usage in megabytes")
class UnitTestResult(BaseModel):
    """Data model for storing unit test execution results to validate code functionality."""
    test_name: str = Field(description="Name of the unit test function")
    passed: bool = Field(description="Whether the test passed successfully")
    error: Optional[str] = Field(default=None, description="Error message if the test failed")
    output: Optional[str] = Field(default=None, description="Output produced by the test")
    test_duration_ms: Optional[float] = Field(default=None, description="Duration of test execution in milliseconds")
    assertion_count: int = Field(default=0, description="Number of assertions executed in the test")
    failure_details: List[str] = Field(default_factory=list, description="Detailed reasons for test failure, if any")
class SelfAnalysisMetrics(BaseModel):
    """Data model for storing self-analysis metrics of internal replicator components."""
    internal_role: str = Field(description="The function's role within the replicator's architecture")
    internal_dependencies: List[str] = Field(description="List of other internal methods called by this function")
    has_ai_dependency: bool = Field(default=False, description="True if this function interacts with an external AI model")
    prompt_template: Optional[str] = Field(default=None, description="The extracted prompt template if an AI dependency exists")
class CognitiveEvaluation(BaseModel):
    """Data model for storing cognitive evaluation metrics of a code block."""
    cyclomatic_complexity: int = Field(description="Cyclomatic complexity of the code block")
    code_smells: List[Dict[str, Any]] = Field(description="List of identified code smells with name and severity")
    violations: List[Dict[str, Any]] = Field(default_factory=list, description="List of identified violations with name and severity")
    is_pure_function: bool = Field(description="Whether the code block is a pure function with no side effects")
    composability_score: float = Field(description="Score indicating how composable the code block is")
    semantic_category: str = Field(description="Semantic category of the code block (e.g., utility, api_service)")
    tags: List[str] = Field(default_factory=list, description="Tags describing the code block's purpose")
    cognitive_score: float = Field(description="Overall cognitive quality score (0.0-1.0)")
    summary: str = Field(default="", description="Natural language summary of the code block")
    test_results: List[UnitTestResult] = Field(default_factory=list, description="Results of unit tests executed on the code block")
    healing_changes: List[str] = Field(default_factory=list, description="List of self-healing changes applied to the code")
    agent_influence: float = Field(default=0.5, description="Influence of AI or agent in analysis (0.0-1.0)")
class CodeBlockPacket(BaseModel):
    """Data model for storing metadata and analysis results of a code block in the SmartReplicatorV4 system."""
    model_config = ConfigDict(extra='forbid')
    segment_id: str = Field(description="Unique identifier for the code block, e.g., module.element.index")
    element_name: str = Field(description="Name of the code element (function or class)")
    element_type: Literal["function", "class", "async_function"] = Field(description="Type of the code element")
    module_id: str = Field(description="Identifier of the module containing the code block")
    source_fingerprint: str = Field(description="SHA256 hash of the code block's source content")
    version: int = Field(default=1, description="Version of the code block for tracking updates")
    timestamp: str = Field(description="ISO timestamp of when the block was analyzed")
    agent_signature: str = Field(description="Signature of the agent or user who analyzed the block")
    is_duplicate_of: Optional[str] = Field(default=None, description="Segment ID of a duplicate block, if any")
    similarity_score: Optional[float] = Field(default=None, description="Similarity score to the duplicate block, if applicable")
    replication_accuracy: Optional[float] = Field(default=None, description="Accuracy of replication compared to original code")
    docstring: str = Field(description="Docstring extracted from the code block")
    start_line: int = Field(description="Starting line number of the code block in the source file")
    end_line: int = Field(description="Ending line number of the code block in the source file")
    code: str = Field(description="Source code of the block")
    formatting: FormattingMetrics = Field(description="Formatting metrics for the code block")
    tokens: TokenUsageMetrics = Field(description="Token usage metrics for the code block")
    sandbox_test: ExecutionSandboxResult = Field(description="Results of sandbox execution for the code block")
    evaluation: CognitiveEvaluation = Field(description="Cognitive evaluation metrics for the code block")
    self_analysis: Optional[SelfAnalysisMetrics] = Field(default=None, description="Self-analysis metrics for internal replicator components")
    source_file_path: Optional[str] = Field(default=None, description="Absolute path to the source file containing the code block")
    external_dependencies: List[str] = Field(default_factory=list, description="List of external libraries or modules used by the code block")
    execution_context: Optional[Dict[str, Any]] = Field(default=None, description="Contextual metadata about the execution environment, e.g., Python version")
# Utility Functions
def generate_unit_test_stub(func_name: str, code: str) -> Tuple[str, List[Dict[str, Any]]]:
    """Generate a unit test stub based on function signature and execute it."""
    reason = f"Generating test stub for {func_name} to validate behavior."
    try:
        tree = ast.parse(code)
        for node in ast.walk(tree):
            if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):
                if node.name == func_name:
                    args = [arg.arg for arg in node.args.args]
                    args_str = ", ".join("1" for _ in args) # Simple default inputs
                    test_code = JINJA_ENV.from_string(UNIT_TEST_TEMPLATE).render(
                        func_name=func_name, args_str=args_str
                    )
                    return test_code, [{'name': f'test_{func_name}', 'args': args_str, 'reason': reason}]
    except Exception as e:
        logging.warning(f"Failed to generate test stub for {func_name}: {e}")
        return "", [{'name': f'test_{func_name}', 'error': str(e), 'reason': f"Failed to parse function: {e}"}]
    return "", [{'name': f'test_{func_name}', 'error': 'Function not found', 'reason': "No matching function found in code"}]
def apply_self_healing(code: str) -> Tuple[str, List[str]]:
    """Apply basic fixes to code and return changes made."""
    reason = "Applying self-healing to fix common syntax and style issues."
    changes = []
    healed_code = code
    if "= =" in healed_code:
        healed_code = healed_code.replace("= =", "==")
        changes.append("Fixed mistaken '= =' to '=='")
    if "if (" in healed_code:
        healed_code = healed_code.replace("if (", "if ").replace("):", ":")
        changes.append("Simplified if condition syntax")
    if "return None" in healed_code:
        healed_code = re.sub(r'\breturn\s+None\b', 'return', healed_code)
        changes.append("Removed redundant 'return None'")
    return healed_code, changes
async def search_similar_blocks(query: str, corpus: List[Dict[str, Any]], sentence_model: Any = None) -> List[Tuple[str, float]]:
    """Rank blocks by semantic similarity using embeddings or TF-IDF fallback."""
    reason = "Performing semantic similarity search to find relevant code blocks."
    if not corpus:
        return []
    try:
        code_texts = [block['code'] for block in corpus]
        if sentence_model:
            try:
                query_embedding, corpus_embeddings = await asyncio.to_thread(
                    lambda q, ct: (sentence_model.encode([q]), sentence_model.encode(ct)),
                    query, code_texts
                )
                scores = cosine_similarity(query_embedding, corpus_embeddings)[0]
                reason = "Used sentence-transformer embeddings for precise semantic matching."
            except Exception as e:
                logging.warning(f"Embedding search failed: {e}, falling back to TF-IDF")
                tfidf = TfidfVectorizer().fit(code_texts + [query])
                vectors = tfidf.transform(code_texts + [query])
                scores = cosine_similarity(vectors[-1], vectors[:-1])[0]
                reason = f"Fell back to TF-IDF due to embedding failure: {e}"
        else:
            tfidf = TfidfVectorizer().fit(code_texts + [query])
            vectors = tfidf.transform(code_texts + [query])
            scores = cosine_similarity(vectors[-1], vectors[:-1])[0]
            reason = "Used TF-IDF for semantic similarity due to no embedding model."
        result = sorted(
            [(block['segment_id'], score) for block, score in zip(corpus, scores)],
            key=lambda x: x[1], reverse=True
        )
        return result
    except Exception as e:
        logging.warning(f"Semantic search failed: {e}")
        return []
def get_user_signature() -> str:
    """Get user or agent signature for attribution."""
    try:
        return os.environ.get('CITADEL_AGENT_ID', f"user:{getpass.getuser()}")
    except Exception:
        return "agent:unknown"
def summarize_code_block(packet: Dict[str, Any]) -> str:
    """Generate a natural language summary of a code block."""
    try:
        name = packet.get('element_name', 'unknown')
        kind = packet.get('element_type', 'function')
        doc = packet.get('docstring', 'No docstring provided')
        score = packet.get('evaluation', {}).get('cognitive_score', 0.0)
        category = packet.get('evaluation', {}).get('semantic_category', 'Uncategorized')
        tags = packet.get('evaluation', {}).get('tags', [])
        tag_str = ", ".join(tags) or "none"
        return (
            f"{kind.capitalize()} '{name}' is categorized as '{category}' with a cognitive score of {score:.2f}. "
            f"It is described as: {doc[:80]}... Tags: {tag_str}"
        )
    except Exception as e:
        logging.warning(f"Failed to summarize block: {e}")
        return f"Error generating summary: {e}"
# Data Persistence Layer
class LedgerEngine:
    """Manages event logging to a SQLite database and JSONL file."""
    def __init__(self, db_path: Path):
        self.db_path = str(db_path) # aiosqlite uses string paths
        self.jsonl_path = str(db_path.parent / "cognitive_ledger.jsonl")
        db_path.parent.mkdir(parents=True, exist_ok=True)
    async def _migrate_schema(self):
        async with aiosqlite.connect(self.db_path) as db:
            await db.execute("PRAGMA journal_mode = WAL;")
            await db.execute("PRAGMA synchronous = FULL;")
            await db.execute("""
                CREATE TABLE IF NOT EXISTS cognitive_ledger (
                    id TEXT PRIMARY KEY,
                    block_id TEXT NOT NULL,
                    timestamp TEXT NOT NULL,
                    event_type TEXT NOT NULL,
                    payload_json TEXT NOT NULL
                );
            """)
            await db.commit()
    async def log_event(self, block_id: str, event_type: str, payload: Dict[str, Any]):
        event_id = hashlib.sha256(f"{block_id}{event_type}{datetime.now(timezone.utc).isoformat()}{os.urandom(4)}".encode()).hexdigest()
        log_entry = {'id': event_id, 'block_id': block_id, 'event_type': event_type, 'payload': payload}
        # Developer Note: Grown to use Citadel log_event_jsonl if available for unified logging.
        if log_event_jsonl:
            log_event_jsonl(Path(self.jsonl_path), log_entry)
        else:
            with open(self.jsonl_path, 'a', encoding='utf-8') as f:
                json.dump(log_entry, f)
                f.write('\n')
        retries = 3
        for attempt in range(retries):
            try:
                async with aiosqlite.connect(self.db_path) as db:
                    await db.execute(
                        "INSERT INTO cognitive_ledger VALUES (?, ?, ?, ?, ?)",
                        (event_id, block_id, datetime.now(timezone.utc).isoformat(), event_type, json.dumps(payload))
                    )
                    await db.commit()
                return event_id
            except aiosqlite.OperationalError as e:
                if "database is locked" in str(e) and attempt < retries - 1:
                    await asyncio.sleep(0.5 * (2 ** attempt)) # Exponential backoff
                    continue
                logging.error(f"Failed to log event after {retries} attempts: {e}")
                return None
    async def get_history(self, block_id: str) -> List[Dict]:
        async with aiosqlite.connect(self.db_path) as db:
            cursor = await db.execute("SELECT id, timestamp, event_type, payload_json FROM cognitive_ledger WHERE block_id = ? ORDER BY timestamp DESC", (block_id,))
            rows = await cursor.fetchall()
            return [{
                'id': row[0], 'timestamp': row[1],
                'event_type': row[2], 'payload': json.loads(row[3])
            } for row in rows]
    def close(self):
        logging.info("LedgerEngine closed")
class LineageManager:
    """Manages code block lineage for tracking derivations and relationships."""
    def __init__(self, db_path: Path):
        self.db_path = str(db_path)
    async def _migrate_schema(self):
        async with aiosqlite.connect(self.db_path) as db:
            await db.execute("PRAGMA journal_mode = WAL;")
            await db.execute("PRAGMA synchronous = FULL;")
            await db.execute("PRAGMA foreign_keys = ON;")
            await db.execute("""
                CREATE TABLE IF NOT EXISTS block_lineage (
                    segment_id TEXT PRIMARY KEY,
                    parent_segment_id TEXT,
                    root_segment_id TEXT,
                    module_id TEXT NOT NULL,
                    element_name TEXT NOT NULL,
                    generation INT DEFAULT 1,
                    relation_type TEXT,
                    timestamp TEXT NOT NULL,
                    notes TEXT,
                    FOREIGN KEY(parent_segment_id) REFERENCES block_lineage(segment_id)
                );
            """)
            await db.commit()
    async def track_derivation(self, segment_id: str, parent_id: Optional[str], module_id: str, element_name: str, notes: str):
        retries = 3
        for attempt in range(retries):
            try:
                async with aiosqlite.connect(self.db_path) as db:
                    now = datetime.now(timezone.utc).isoformat()
                    root_segment_id = parent_id or segment_id
                    generation = 1
                    if parent_id:
                        cursor = await db.execute(
                            "SELECT root_segment_id, generation FROM block_lineage WHERE segment_id = ?",
                            (parent_id,)
                        )
                        parent = await cursor.fetchone()
                        if parent:
                            root_segment_id = parent[0]
                            generation = parent[1] + 1
                    await db.execute("""
                        INSERT OR IGNORE INTO block_lineage (
                            segment_id, parent_segment_id, root_segment_id,
                            module_id, element_name, generation,
                            relation_type, timestamp, notes
                        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)
                    """, (
                        segment_id, parent_id, root_segment_id,
                        module_id, element_name, generation,
                        "derived" if parent_id else "direct", now, notes
                    ))
                    await db.commit()
                return
            except aiosqlite.OperationalError as e:
                if "database is locked" in str(e) and attempt < retries - 1:
                    await asyncio.sleep(0.5 * (2 ** attempt))
                    continue
                logging.error(f"Failed to track derivation after {retries} attempts: {e}")
                raise
    async def get_lineage(self, segment_id: str) -> List[Dict]:
        async with aiosqlite.connect(self.db_path) as db:
            cursor = await db.execute("""
                WITH RECURSIVE lineage(segment_id, parent_segment_id, root_segment_id,
                                      module_id, element_name, generation, relation_type,
                                      timestamp, notes) AS (
                    SELECT segment_id, parent_segment_id, root_segment_id,
                           module_id, element_name, generation, relation_type,
                           timestamp, notes
                    FROM block_lineage WHERE segment_id = ?
                    UNION
                    SELECT b.segment_id, b.parent_segment_id, b.root_segment_id,
                           b.module_id, b.element_name, b.generation, b.relation_type,
                           b.timestamp, b.notes
                    FROM block_lineage b
                    JOIN lineage l ON b.segment_id = l.parent_segment_id
                )
                SELECT * FROM lineage ORDER BY generation DESC;
            """, (segment_id,))
            rows = await cursor.fetchall()
            return [dict(zip(['segment_id', 'parent_segment_id', 'root_segment_id', 'module_id', 'element_name', 'generation', 'relation_type', 'timestamp', 'notes'], row)) for row in rows]
    def close(self):
        logging.info("LineageManager closed")
class SmartBankManager:
    """Manages storage and retrieval of code block packets in a SQLite database and JSONL file."""
    def __init__(self, db_path: Path, ledger: LedgerEngine, lineage: LineageManager):
        self.db_path = str(db_path)
        self.ledger = ledger
        self.lineage = lineage
        self.jsonl_path = str(db_path.parent / "pipeline_output.jsonl")
        self.console = Console()
    async def _migrate_schema(self):
        async with aiosqlite.connect(self.db_path) as db:
            await db.execute("""
                CREATE TABLE IF NOT EXISTS code_packets (
                    segment_id TEXT PRIMARY KEY,
                    module_id TEXT NOT NULL,
                    element_name TEXT NOT NULL,
                    element_type TEXT NOT NULL,
                    cognitive_score REAL,
                    timestamp TEXT,
                    packet_json TEXT NOT NULL
                );
            """)
            await db.execute("""
                CREATE TABLE IF NOT EXISTS module_fingerprints (
                    module_id TEXT PRIMARY KEY,
                    fingerprint TEXT NOT NULL,
                    last_scanned TEXT
                );
            """)
            await db.commit()
    async def store_block(self, packet: CodeBlockPacket):
        packet_json = packet.model_dump_json()
        log_payload = {
            'module_id': packet.module_id,
            'element_name': packet.element_name,
            'cognitive_score': packet.evaluation.cognitive_score,
            'reason': f"Stored block {packet.module_id}.{packet.element_name}"
        }
        log_entry = {'module_id': packet.module_id, 'event_type': 'store_block', 'payload': log_payload}
        # Developer Note: Grown to use Citadel log_event_jsonl if available.
        if log_event_jsonl:
            log_event_jsonl(Path(self.jsonl_path), log_entry)
        else:
            with open(self.jsonl_path, 'a', encoding='utf-8') as f:
                json.dump(log_entry, f)
                f.write('\n')
        retries = 3
        for attempt in range(retries):
            try:
                async with aiosqlite.connect(self.db_path) as db:
                    cursor = await db.execute("SELECT segment_id FROM code_packets WHERE segment_id = ?", (packet.segment_id,))
                    if await cursor.fetchone():
                        logging.info(f"Skipping duplicate packet for {packet.segment_id}")
                        return
                    await db.execute(
                        "INSERT OR REPLACE INTO code_packets VALUES (?, ?, ?, ?, ?, ?, ?)",
                        (
                            packet.segment_id, packet.module_id, packet.element_name,
                            packet.element_type, packet.evaluation.cognitive_score,
                            packet.timestamp, packet_json
                        )
                    )
                    await db.commit()
                self._save_to_jsonl({
                    'segment_id': packet.segment_id,
                    'module_id': packet.module_id,
                    'element_name': packet.element_name,
                    'timestamp': packet.timestamp,
                    'packet': packet.model_dump()
                })
                await self.ledger.log_event(packet.segment_id, 'store_block', log_payload)
                return
            except aiosqlite.OperationalError as e:
                if "database is locked" in str(e) and attempt < retries - 1:
                    await asyncio.sleep(0.5 * (2 ** attempt))
                    continue
                logging.error(f"Failed to store block after {retries} attempts: {e}")
                raise
    def _save_to_jsonl(self, packet: Dict[str, Any]):
        try:
            with open(self.jsonl_path, 'a', encoding='utf-8') as f:
                json.dump(packet, f)
                f.write('\n')
            logging.info(f"Saved packet to {self.jsonl_path}")
        except Exception as e:
            logging.error(f"Failed to save JSONL: {e}")
    async def get_last_module_fingerprint(self, module_id: str) -> Optional[str]:
        async with aiosqlite.connect(self.db_path) as db:
            cursor = await db.execute("SELECT fingerprint FROM module_fingerprints WHERE module_id = ?", (module_id,))
            row = await cursor.fetchone()
            return row[0] if row else None
    async def update_module_fingerprint(self, module_id: str, fingerprint: str):
        retries = 3
        for attempt in range(retries):
            try:
                async with aiosqlite.connect(self.db_path) as db:
                    await db.execute(
                        "INSERT OR REPLACE INTO module_fingerprints (module_id, fingerprint, last_scanned) VALUES (?, ?, ?)",
                        (module_id, fingerprint, datetime.now(timezone.utc).isoformat())
                    )
                    await db.commit()
                return
            except aiosqlite.OperationalError as e:
                if "database is locked" in str(e) and attempt < retries - 1:
                    await asyncio.sleep(0.5 * (2 ** attempt))
                    continue
                logging.error(f"Failed to update fingerprint after {retries} attempts: {e}")
                raise
    async def get_all_blocks(self) -> List[Dict]:
        async with aiosqlite.connect(self.db_path) as db:
            cursor = await db.execute("SELECT packet_json FROM code_packets")
            rows = await cursor.fetchall()
            return [json.loads(row[0]) for row in rows]
    async def get_all_blocks_summary(self) -> List[Dict]:
        async with aiosqlite.connect(self.db_path) as db:
            cursor = await db.execute("SELECT module_id, element_name, cognitive_score, element_type FROM code_packets ORDER BY cognitive_score DESC")
            rows = await cursor.fetchall()
            return [dict(zip(['module_id', 'element_name', 'cognitive_score', 'element_type'], row)) for row in rows]
    def close(self):
        logging.info("SmartBankManager closed")
PROJECT_ROOT = Path(r"D:\MASTER_CITADEL\CITADEL_LLM")
EVAL_DIR = PROJECT_ROOT / "smart_bank" / "evaluation_modules"
def generate_eval_stub(meta: Dict[str, Any], import_path: str) -> str:
    """Generates an evaluation stub for a module."""
    timestamp = datetime.now(timezone.utc).isoformat()
    return f"""# smart_bank/evaluation_modules/{meta['module_name']}_eval.py
# --- AIMB Block ---
# === REPLICATED VIA SMARTREPLICATORV4 ===
# _module_id: {meta['module_name'].upper()}_EVAL
# _linked_code: {meta['path']}
# _fingerprint: {meta['fingerprint']}
# _purpose: {meta.get('aimb', {}).get('_purpose', 'Evaluation logic for the module')}
# _generated: {timestamp}
def evaluate() -> dict:
    try:
        import {import_path}
        return {{
            "module_id": "{meta['module_name'].upper()}",
            "domain": "{meta.get('aimb', {}).get('_domain', 'memory_systems')}",
            "status": "Operational",
            "accuracy_score": 0.0,
            "last_test_passed": False,
            "timestamp": "{timestamp}",
            "critical_findings": [],
            "aimb": {{
                "_page_id": "{meta['module_name'].upper()}_EVAL",
                "_execution_role": "evaluation",
                "_linked_code": "{meta['path']}"
            }},
            "runtime_metrics": {{
                "avg_latency_ms": 0.0,
                "cpu_usage_percent": 0.0,
                "memory_mb": 0.0,
                "exception_count": 0
            }}
        }}
    except Exception as e:
        return {{
            "module_id": "{meta['module_name'].upper()}",
            "domain": "{meta.get('aimb', {}).get('_domain', 'memory_systems')}",
            "status": "Error",
            "error": str(e),
            "timestamp": "{timestamp}",
            "critical_findings": ["Evaluation failed"],
            "aimb": {{
                "_page_id": "{meta['module_name'].upper()}_EVAL",
                "_execution_role": "evaluation",
                "_linked_code": "{meta['path']}"
            }}
        }}
"""
def extract_advanced_metadata(file_path: Path, project_root: Path) -> Dict:
    try:
        code = file_path.read_text('utf-8')
        fingerprint = hashlib.sha256(code.encode('utf-8')).hexdigest()
        module_name = file_path.stem
        path = str(file_path.relative_to(project_root))
        aimb = {}
        for line in code.splitlines():
            if line.startswith('# _'):
                key, value = line[2:].split(':', 1)
                aimb[key.strip()] = value.strip()
        return {
            'module_name': module_name,
            'path': path,
            'fingerprint': fingerprint,
            'aimb': aimb
        }
    except Exception as e:
        return {'error': str(e)}
def resolve_import_path(file_path: Path, project_root: Path) -> str:
    rel_path = file_path.relative_to(project_root)
    parts = rel_path.with_suffix('').parts
    return '.'.join(parts)
class FunctionBank:
    def __init__(self):
        self.functions = []
        self.client = OpenAI() if os.getenv("OPENAI_API_KEY") else None
    def evaluate_and_review(self, file_path, project_root):
        console = Console()
        if self.client:
            try:
                code = file_path.read_text('utf-8')
                prompt = f"Review this Python code, provide suggestions for improvements, then provide the improved code in ```python ... ``` block.\n{code[:8000]}" # limit length
                response = self.client.chat.completions.create(
                    model="gpt-4o-mini",
                    messages=[{"role": "user", "content": prompt}],
                )
                content = response.choices[0].message.content
                console.print(f"AI Review for {file_path}:\n{content}")
                improved_match = re.search(r'```python\n(.*?)\n```', content, re.DOTALL)
                if improved_match:
                    improved_code = improved_match.group(1).strip()
                    improved_path = file_path.parent / f"{file_path.stem}_improved.py"
                    improved_path.write_text(improved_code, 'utf-8')
                    console.print(f"[green]Generated improved code at {improved_path}[/green]")
            except Exception as e:
                console.print(f"[red]Failed to review {file_path}: {e}[/red]")
        else:
            console.print(f"[yellow]Evaluated and reviewed {file_path} (no AI available)[/yellow]")
def scan_and_replicate(source_dirs: List[str], project_root: Path = PROJECT_ROOT, bank: FunctionBank = None):
    """Scans source files and updates the function bank with intelligent replication."""
    console = Console()
    if bank is None:
        bank = FunctionBank()
    for source_dir in source_dirs:
        source_path = project_root / source_dir
        if not source_path.exists():
            console.print(f"[red]Directory not found: {source_path}[/red]")
            continue
        for file_path in source_path.rglob("*.py"):
            if "_eval" in file_path.name:
                continue
            meta = extract_advanced_metadata(file_path, project_root)
            if "error" in meta:
                console.print(f"[red]Error extracting metadata for {file_path}: {meta['error']}[/red]")
                continue
            import_path = resolve_import_path(file_path, project_root)
            eval_content = generate_eval_stub(meta, import_path)
            eval_path = EVAL_DIR / f"{meta['module_name']}_eval.py"
            eval_path.parent.mkdir(exist_ok=True)
            try:
                eval_path.write_text(eval_content, encoding="utf-8")
                console.print(f"[green]Generated eval stub: {eval_path}[/green]")
            except Exception as e:
                console.print(f"[red]Failed to write eval stub {eval_path}: {e}[/red]")
            bank.evaluate_and_review(file_path, project_root)
class SmartReplicatorV4:
    """Main class for orchestrating code analysis, replication, and persistence in the SmartReplicatorV4 system."""
    def __init__(self, config: Dict, bank_manager: SmartBankManager):
        self.config = config
        self.bank_manager = bank_manager
        self.ledger = bank_manager.ledger
        self.lineage = bank_manager.lineage
        self.console = Console()
        self.source_code_paths = config.get('source_code_paths', [])
        self.eval_dir = Path(config.get('evaluation_modules_dir', 'smart_bank/evaluation_modules'))
        self.eval_dir.mkdir(parents=True, exist_ok=True)
        self.last_run_packets: List[CodeBlockPacket] = []
        self.openai_client = OpenAI() if OpenAI and os.getenv("OPENAI_API_KEY") else None
        self.sentence_model = SentenceTransformer('all-MiniLM-L6-v2') if 'sentence_transformers' in sys.modules else None
    def _compute_hash(self, text: str) -> str:
        return hashlib.sha256(text.encode('utf-8')).hexdigest()
    async def _log_ai_metric(self, action: str, block_id: str, reason: str, inputs: Dict = {}, outputs: Dict = {}):
        await self.ledger.log_event(block_id, 'ai_metric', {
            'action': action, 'reason': reason, 'inputs': inputs, 'outputs': outputs,
            'timestamp': datetime.now(timezone.utc).isoformat()
        })
    async def discover_modules_to_replicate(self, external_path: Optional[str] = None) -> List[Path]:
        reason = f"Discovering modules to replicate, external path: {external_path or 'none'}."
        files_to_audit = []
        ignore_patterns = ['__pycache__', 'venv', '.venv', 'tests', 'node_modules']
        paths = self.source_code_paths[:]
        if external_path:
            external_file = Path(external_path)
            if external_file.exists() and external_file.suffix == '.py':
                paths.append(external_file)
        tasks = []
        for s_dir_str in paths:
            source_dir = Path(s_dir_str) if isinstance(s_dir_str, str) else s_dir_str
            if not source_dir.exists():
                continue
            if source_dir.is_file():
                if source_dir.suffix == '.py' and not any(p in source_dir.parts for p in ignore_patterns):
                    tasks.append(self._check_file_fingerprint(source_dir))
                continue
            for py_file in source_dir.rglob("*.py"):
                if any(part in py_file.parts for part in ignore_patterns):
                    continue
                tasks.append(self._check_file_fingerprint(py_file))
        results = await asyncio.gather(*tasks, return_exceptions=True)
        for result_item in results:
            if isinstance(result_item, Exception):
                logging.error(f"Error during file discovery task: {result_item}")
                continue
            py_file, result_path = result_item
            if isinstance(result_path, Path):
                files_to_audit.append(result_path)
        await self._log_ai_metric(
            action='discover_modules',
            block_id='system',
            inputs={'paths': [str(p) for p in paths], 'external_path': str(external_path) if external_path else None},
            outputs={'files_to_audit': [str(f) for f in files_to_audit]},
            reason=reason
        )
        return files_to_audit
    async def _check_file_fingerprint(self, py_file: Path) -> Tuple[Path, Optional[Path]]:
        reason = f"Checking fingerprint for {py_file} to detect changes."
        try:
            source_code = await asyncio.to_thread(py_file.read_text, 'utf-8')
            current_fingerprint = self._compute_hash(source_code)
            last_fingerprint = await self.bank_manager.get_last_module_fingerprint(py_file.stem)
            if not last_fingerprint or current_fingerprint != last_fingerprint:
                await self._log_ai_metric(
                    action='fingerprint_check',
                    block_id=py_file.stem,
                    inputs={'file': str(py_file), 'current_fingerprint': current_fingerprint},
                    outputs={'changed': True},
                    reason=reason
                )
                return py_file, py_file
            await self._log_ai_metric(
                action='fingerprint_check',
                block_id=py_file.stem,
                inputs={'file': str(py_file), 'current_fingerprint': current_fingerprint},
                outputs={'changed': False},
                reason=reason
            )
            return py_file, None
        except Exception as e:
            logging.error(f"Failed to process {py_file}: {e}")
            await self._log_ai_metric(
                action='fingerprint_check',
                block_id=py_file.stem,
                inputs={'file': str(py_file)},
                outputs={'error': str(e)},
                reason=f"Failed to compute fingerprint: {e}"
            )
            return py_file, None
    async def _analyze_formatting(self, code: str) -> FormattingMetrics:
        reason = "Analyzing code formatting to assess structure and readability."
        lines = code.splitlines()
        line_lengths = [len(line) for line in lines]
        indent_levels = {len(line) - len(line.lstrip(' \t')): 0 for line in lines}
        comment_count = sum(1 for line in lines if line.strip().startswith('#'))
        blank_line_count = sum(1 for line in lines if not line.strip())
        whitespace_count = sum(line.count(' ') + line.count('\t') for line in lines)
        mean_line_length = sum(line_lengths) / len(lines) if lines else 0
        line_length_variance = sum((length - mean_line_length) ** 2 for length in line_lengths) / len(lines) if lines else 0
        max_consecutive_blanks = 0
        current_consecutive_blanks = 0
        for line in lines:
            if not line.strip():
                current_consecutive_blanks += 1
                max_consecutive_blanks = max(max_consecutive_blanks, current_consecutive_blanks)
            else:
                current_consecutive_blanks = 0
            indent = len(line) - len(line.lstrip(' \t'))
            indent_levels[indent] += 1
        metrics = FormattingMetrics(
            line_count=len(lines),
            char_count=len(code),
            avg_line_length=round(mean_line_length, 2) if lines else 0,
            max_line_length=max(line_lengths, default=0),
            indent_distribution=indent_levels,
            comment_count=comment_count,
            blank_line_count=blank_line_count,
            line_length_variance=round(line_length_variance, 2),
            whitespace_ratio=round(whitespace_count / len(code), 3) if len(code) > 0 else 0.0,
            max_consecutive_blank_lines=max_consecutive_blanks
        )
        await self._log_ai_metric(
            action='analyze_formatting',
            block_id='formatting',
            inputs={'code_length': len(code)},
            outputs=metrics.model_dump(),
            reason=reason
        )
        return metrics
    async def _analyze_token_usage(self, code: str) -> TokenUsageMetrics:
        reason = "Analyzing token usage to evaluate code complexity."
        import keyword
        operators = ['+', '-', '*', '/', '//', '%', '**', '==', '!=', '<', '>', '<=', '>=', 'and', 'or', 'not']
        operator_count = sum(code.count(op) for op in operators)
        keyword_count = sum(code.count(f' {kw} ') for kw in keyword.kwlist)
        identifiers = len(set(re.findall(r'\b[a-zA-Z_][a-zA-Z0-9_]*\b', code)))
        total_tokens = sum([code.count('(') + code.count(')'), code.count('{') + code.count('}'),
                            code.count('[') + code.count(']'), code.count('.'), code.count("'") + code.count('"') +
                            code.count(r"'''") + code.count(r'"""'), operator_count, keyword_count, identifiers])
        metrics = TokenUsageMetrics(
            parentheses=code.count('(') + code.count(')'),
            braces=code.count('{') + code.count('}'),
            brackets=code.count('[') + code.count(']'),
            dots=code.count('.'),
            quotes=code.count("'") + code.count('"') + code.count(r"'''") + code.count(r'"""'),
            operators=operator_count,
            keywords=keyword_count,
            identifier_count=identifiers,
            token_density=round(total_tokens / len(code), 3) if len(code) > 0 else 0.0
        )
        await self._log_ai_metric(
            action='analyze_tokens',
            block_id='tokens',
            inputs={'code_length': len(code)},
            outputs=metrics.model_dump(),
            reason=reason
        )
        return metrics
    @contextmanager
    def _safe_sandbox(self):
        restricted_globals = {
            '__builtins__': {k: v for k, v in __builtins__.__dict__.items() if k in (
                'abs', 'len', 'max', 'min', 'sum', 'range', 'int', 'float', 'str', 'list', 'dict', 'tuple'
            )}
        }
        yield restricted_globals
    async def _test_in_sandbox(self, code_segment: str, block_id: str) -> ExecutionSandboxResult:
        reason = f"Executing {block_id} in sandbox to assess runtime behavior."
        start_time = time.monotonic()
        process = psutil.Process()
        try:
            with self._safe_sandbox() as sandbox:
                compiled_code = compile(code_segment, '<sandbox>', 'exec')
                await asyncio.wait_for(
                    asyncio.to_thread(exec, compiled_code, sandbox),
                    timeout=1.0
                )
                fn_names = [k for k, v in sandbox.items() if callable(v) and not k.startswith('__')]
                if not fn_names:
                    result = ExecutionSandboxResult(
                        executed=True,
                        execution_time_ms=(time.monotonic() - start_time) * 1000,
                        memory_usage_mb=process.memory_info().rss / (1024 * 1024)
                    )
                    await self._log_ai_metric(
                        action='sandbox_execution',
                        block_id=block_id,
                        inputs={'code': code_segment[:100]},
                        outputs=result.model_dump(),
                        reason=reason
                    )
                    return result
                fn = sandbox[fn_names[0]]
                output = await asyncio.wait_for(
                    asyncio.to_thread(fn if not isinstance(fn, type) else fn()),
                    timeout=1.0
                )
                result = ExecutionSandboxResult(
                    executed=True,
                    return_type=str(type(output)),
                    execution_time_ms=(time.monotonic() - start_time) * 1000,
                    memory_usage_mb=process.memory_info().rss / (1024 * 1024)
                )
                await self._log_ai_metric(
                    action='sandbox_execution',
                    block_id=block_id,
                    inputs={'code': code_segment[:100]},
                    outputs=result.model_dump(),
                    reason=reason
                )
                return result
        except asyncio.TimeoutError:
            result = ExecutionSandboxResult(
                executed=False,
                error="Timeout: Execution exceeded 1 second",
                execution_time_ms=(time.monotonic() - start_time) * 1000,
                memory_usage_mb=process.memory_info().rss / (1024 * 1024)
            )
            await self._log_ai_metric(
                action='sandbox_execution',
                block_id=block_id,
                inputs={'code': code_segment[:100]},
                outputs=result.model_dump(),
                reason=f"Execution timed out: {result.error}"
            )
            return result
        except Exception as e:
            result = ExecutionSandboxResult(
                executed=False,
                error=f"{type(e).__name__}: {e}",
                execution_time_ms=(time.monotonic() - start_time) * 1000,
                memory_usage_mb=process.memory_info().rss / (1024 * 1024)
            )
            await self._log_ai_metric(
                action='sandbox_execution',
                block_id=block_id,
                inputs={'code': code_segment[:100]},
                outputs=result.model_dump(),
                reason=f"Execution failed: {result.error}"
            )
            return result
    async def _run_unit_test(self, test_code: str, block_id: str) -> UnitTestResult:
        reason = f"Running unit test for {block_id} to validate functionality."
        start_time = time.monotonic()
        try:
            with self._safe_sandbox() as sandbox:
                compiled_code = compile(test_code, '<test>', 'exec')
                await asyncio.wait_for(
                    asyncio.to_thread(exec, compiled_code, sandbox),
                    timeout=1.0
                )
                fn_names = [k for k, v in sandbox.items() if callable(v) and k.startswith('test_')]
                if not fn_names:
                    result = UnitTestResult(
                        test_name='basic_test',
                        passed=False,
                        error='No test function found',
                        test_duration_ms=(time.monotonic() - start_time) * 1000,
                        assertion_count=0,
                        failure_details=['No test function defined in the test code']
                    )
                    await self._log_ai_metric(
                        action='unit_test',
                        block_id=block_id,
                        inputs={'test_code': test_code[:100]},
                        outputs=result.model_dump(),
                        reason=f"Test failed: {result.error}"
                    )
                    return result
                fn = sandbox[fn_names[0]]
                assertion_count = len(re.findall(r'\bassert\b', test_code))
                output = await asyncio.wait_for(
                    asyncio.to_thread(fn),
                    timeout=1.0
                )
                result = UnitTestResult(
                    test_name=fn_names[0],
                    passed=True,
                    output=str(output) if output else "None",
                    test_duration_ms=(time.monotonic() - start_time) * 1000,
                    assertion_count=assertion_count,
                    failure_details=[]
                )
                await self._log_ai_metric(
                    action='unit_test',
                    block_id=block_id,
                    inputs={'test_code': test_code[:100]},
                    outputs=result.model_dump(),
                    reason=reason
                )
                return result
        except asyncio.TimeoutError:
            result = UnitTestResult(
                test_name='basic_test',
                passed=False,
                error="Timeout: Test execution exceeded 1 second",
                test_duration_ms=(time.monotonic() - start_time) * 1000,
                assertion_count=len(re.findall(r'\bassert\b', test_code)),
                failure_details=['Test execution timed out after 1 second']
            )
            await self._log_ai_metric(
                action='unit_test',
                block_id=block_id,
                inputs={'test_code': test_code[:100]},
                outputs=result.model_dump(),
                reason=f"Test failed: {result.error}"
            )
            return result
        except Exception as e:
            result = UnitTestResult(
                test_name='basic_test',
                passed=False,
                error=f"{type(e).__name__}: {e}",
                test_duration_ms=(time.monotonic() - start_time) * 1000,
                assertion_count=len(re.findall(r'\bassert\b', test_code)),
                failure_details=[f"Exception occurred: {type(e).__name__}: {e}"]
            )
            await self._log_ai_metric(
                action='unit_test',
                block_id=block_id,
                inputs={'test_code': test_code[:100]},
                outputs=result.model_dump(),
                reason=f"Test failed: {result.error}"
            )
            return result
    async def _evaluate_cognitive_metrics(self, code: str, fmt: FormattingMetrics, docstring: str, block_id: str) -> CognitiveEvaluation:
        reason = f"Evaluating cognitive metrics for {block_id} to assess quality and purpose."
        lines = [l.strip() for l in code.splitlines() if l.strip()]
        complexity = sum(line.count(kw) for line in lines for kw in ('if ', 'for ', 'while ', 'and ', 'or ', 'except '))
        smells = []
        violations = []
        tags = []
        if fmt.max_line_length > 100: smells.append({'name': 'long_line', 'severity': 'medium'})
        if fmt.line_count > 40: smells.append({'name': 'long_method', 'severity': 'medium'})
        if complexity > 8: smells.append({'name': 'high_complexity', 'severity': 'high'})
        if 'return None' in code: smells.append({'name': 'explicit_none_return', 'severity': 'low'})
        if any(kw in code for kw in ('eval(', 'exec(')): violations.append({'name': 'unsafe_code', 'severity': 'critical'})
        is_pure = not any(kw in code for kw in ('open(', 'print(', 'input(', 'global ', 'os.', 'sys.', 'requests.'))
        composability = 0.5 * is_pure + 0.3 * (1 - len(smells) / 4) + 0.2 * (1 if docstring else 0)
        category = 'data_model' if 'BaseModel' in code else 'utility'
        if any(k in code for k in ('requests.', 'http.', 'api.')): category = 'api_service'
        elif any(k in docstring.lower() for k in ('llm', 'prompt', 'agent')): category = 'llm_orchestration'
        elif any(k in code for k in ('db.', 'sql', 'session.')): category = 'data_access'
        elif any(k in code for k in ('open(', 'write(')): category = 'io_operation'
        elif any(k in code for k in ('if ', 'for ', 'while ')): category = 'logic'
        ai_score = 0.5
        if self.openai_client:
            try:
                prompt = f"""
Analyze this Python code for semantic purpose, code smells, violations, and tags. Return JSON with:
- category (e.g., utility, api_service, data_access, llm_orchestration, io_operation, logic)
- smells (list of {{name: str, severity: str}} objects, severity: low/medium/high)
- violations (list of {{name: str, severity: str}} objects, e.g., unsafe_code)
- tags (list of strings for open vocabulary)
- score (float 0.0-1.0)
```python
{code}
```
Docstring: {docstring}
"""
                response = await asyncio.to_thread(
                    self.openai_client.chat.completions.create,
                    model="gpt-4o-mini",
                    messages=[{"role": "user", "content": prompt}],
                    response_format={"type": "json_object"},
                    temperature=0.3,
                    max_tokens=200
                )
                ai_result = json.loads(response.choices[0].message.content)
                category = ai_result.get('category', category)
                smells.extend(ai_result.get('smells', []))
                violations.extend(ai_result.get('violations', []))
                tags.extend(ai_result.get('tags', []))
                ai_score = ai_result.get('score', 0.5)
                await self._log_ai_metric(
                    action='ai_annotation',
                    block_id=block_id,
                    inputs={'code': code[:100], 'docstring': docstring[:50]},
                    outputs=ai_result,
                    reason=f"OpenAI analysis for {block_id} to enhance semantic understanding."
                )
            except Exception as e:
                logging.warning(f"OpenAI analysis failed: {e}")
                await self._log_ai_metric(
                    action='ai_annotation',
                    block_id=block_id,
                    inputs={'code': code[:100], 'docstring': docstring[:50]},
                    outputs={'error': str(e)},
                    reason=f"OpenAI analysis failed: {e}"
                )
                ai_score = 0.5 * (1 if docstring else 0.3) + 0.5 * (1 - len(smells) / 5)
        cognitive_score = 0.3 * (1 if docstring else 0) + 0.3 * max(0, 1 - complexity / 10) + 0.2 * composability + 0.2 * ai_score
        eval_result = CognitiveEvaluation(
            cyclomatic_complexity=complexity,
            code_smells=smells,
            violations=violations,
            is_pure_function=is_pure,
            composability_score=round(composability, 3),
            semantic_category=category,
            tags=tags,
            cognitive_score=round(max(0, cognitive_score), 3),
            summary="",
            agent_influence=0.5
        )
        await self._log_ai_metric(
            action='cognitive_evaluation',
            block_id=block_id,
            inputs={'code': code[:100], 'docstring': docstring[:50], 'formatting': fmt.model_dump()},
            outputs=eval_result.model_dump(),
            reason=reason
        )
        return eval_result
    def _perform_self_analysis(self, packet: CodeBlockPacket) -> Optional[SelfAnalysisMetrics]:
        if packet.module_id != THIS_MODULE_FILENAME:
            return None
        code = packet.code
        role = "unknown"
        if "analyze" in packet.element_name or "evaluate" in packet.element_name:
            role = "cognitive_analysis"
        elif "sandbox" in packet.element_name or "test" in packet.element_name:
            role = "runtime_validation"
        elif "log" in packet.element_name or "Engine" in packet.element_name:
            role = "cognitive_logging"
        elif "generate" in packet.element_name or "template" in packet.element_name:
            role = "artifact_generation"
        elif "api" in packet.element_name or "main" in packet.element_name:
            role = "external_interface"
        dependencies = re.findall(r'\self\.(_\w+)\(', code)
        has_ai = "openai" in code.lower()
        prompt_match = re.search(r'prompt\s*=\s*f?"""(.*?)"""', code, re.DOTALL)
        prompt = prompt_match.group(1).strip() if prompt_match else None
        return SelfAnalysisMetrics(
            internal_role=role,
            internal_dependencies=sorted(list(set(dependencies))),
            has_ai_dependency=has_ai,
            prompt_template=prompt
        )
    async def _generate_dynamic_aimb_header(self, packet: CodeBlockPacket) -> str:
        reason = f"Generating AIMB header for {packet.element_name} to document analysis."
        docstring_for_template = "\n# ".join(packet.docstring.splitlines()) if packet.docstring else "No docstring provided."
        warnings = ", ".join(f"{s['name']} ({s['severity']})" for s in packet.evaluation.code_smells) or "None"
        violations = ", ".join(f"{v['name']} ({v['severity']})" for v in packet.evaluation.violations) or "None"
        tags = ", ".join(packet.evaluation.tags) or "None"
        test_results = ", ".join(f"{t.test_name}: {'PASS' if t.passed else 'FAIL'}" for t in packet.evaluation.test_results) or "None"
        header = JINJA_ENV.from_string(AIMB_TEMPLATE).render(
            element_name=packet.element_name,
            element_type=packet.element_type,
            cognitive_score=packet.evaluation.cognitive_score,
            semantic_category=packet.evaluation.semantic_category,
            purity_status="PURE" if packet.evaluation.is_pure_function else "IMPURE",
            segment_id=packet.segment_id,
            module_id=packet.module_id,
            source_fingerprint=packet.source_fingerprint,
            version=packet.version,
            timestamp=packet.timestamp,
            agent_signature=packet.agent_signature,
            agent_influence=packet.evaluation.agent_influence,
            cyclomatic_complexity=packet.evaluation.cyclomatic_complexity,
            composability_score=packet.evaluation.composability_score,
            warnings=warnings,
            violations=violations,
            tags=tags,
            test_results=test_results,
            summary=packet.evaluation.summary,
            docstring_for_template=docstring_for_template,
            self_analysis=packet.self_analysis
        ).strip()
        await self._log_ai_metric(
            action='header_generation',
            block_id=f"{packet.module_id}.{packet.element_name}",
            inputs={'packet': packet.model_dump()},
            outputs={'header': header[:100]},
            reason=reason
        )
        return header
    async def _compute_replication_accuracy(self, original_code: str, replicated_code: str) -> float:
        reason = "Computing replication accuracy to measure fidelity."
        accuracy = round(difflib.SequenceMatcher(None, original_code, replicated_code).ratio(), 4)
        await self._log_ai_metric(
            action='replication_accuracy',
            block_id='replication',
            inputs={'original_length': len(original_code), 'replicated_length': len(replicated_code)},
            outputs={'accuracy': accuracy},
            reason=reason
        )
        return accuracy
    async def _process_block(self, node: ast.AST, source_code: str, source_fingerprint: str, module_id: str, idx: int, source_path: Path) -> CodeBlockPacket:
        reason = f"Processing AST node {node.name} to create cognitive packet."
        element_name = node.name # Define here for use in except block
        try:
            element_type = 'class' if isinstance(node, ast.ClassDef) else 'async_function' if isinstance(node, ast.AsyncFunctionDef) else 'function'
            segment_id = f"{module_id}.{element_name}.{idx}"
            try:
                code_segment = ast.get_source_segment(source_code, node)
            except AttributeError as e:
                logging.warning(f"Missing location info for {element_name}: {e}")
                code_segment = ""
                start_line = 0
                end_line = 0
            else:
                start_line = node.lineno
                end_line = node.end_lineno
            if not code_segment:
                raise ValueError("Failed to extract source segment")
            healed_code, healing_changes = apply_self_healing(code_segment)
            docstring = ast.get_docstring(node) or ""
            fmt_metrics = await self._analyze_formatting(healed_code)
            token_metrics = await self._analyze_token_usage(healed_code)
            sandbox_result = await self._test_in_sandbox(healed_code, segment_id)
            test_code, test_cases = generate_unit_test_stub(element_name, healed_code)
            test_results = []
            if test_code:
                test_results.append(await self._run_unit_test(test_code, segment_id))
            cognitive_eval = await self._evaluate_cognitive_metrics(healed_code, fmt_metrics, docstring, segment_id)
            cognitive_eval.test_results = test_results
            cognitive_eval.healing_changes = healing_changes
            cognitive_eval.summary = summarize_code_block({
                'element_name': element_name,
                'element_type': element_type,
                'docstring': docstring,
                'evaluation': cognitive_eval.model_dump()
            })
            all_blocks = await self.bank_manager.get_all_blocks()
            similar_blocks = await search_similar_blocks(healed_code, all_blocks, self.sentence_model)
            is_duplicate_of = None
            similarity_score = None
            if similar_blocks and similar_blocks[0][1] > 0.95:
                is_duplicate_of = similar_blocks[0][0]
                similarity_score = similar_blocks[0][1]
            tree = ast.parse(healed_code)
            external_deps = []
            for import_node in ast.walk(tree):
                if isinstance(import_node, ast.Import):
                    external_deps.extend(m.name for m in import_node.names if m.name not in ('sys', 'os', 'builtins'))
                elif isinstance(import_node, ast.ImportFrom):
                    if import_node.module not in ('sys', 'os', 'builtins'):
                        external_deps.append(import_node.module)
            external_deps = sorted(list(set(external_deps)))
            packet = CodeBlockPacket(
                segment_id=segment_id,
                element_name=element_name,
                element_type=element_type,
                module_id=module_id,
                source_fingerprint=self._compute_hash(healed_code),
                version=1,
                timestamp=datetime.now(timezone.utc).isoformat(),
                agent_signature=get_user_signature(),
                is_duplicate_of=is_duplicate_of,
                similarity_score=similarity_score,
                replication_accuracy=None,
                docstring=docstring,
                start_line=start_line,
                end_line=end_line,
                code=healed_code,
                formatting=fmt_metrics,
                tokens=token_metrics,
                sandbox_test=sandbox_result,
                evaluation=cognitive_eval,
                self_analysis=self._perform_self_analysis(
                    CodeBlockPacket(
                        segment_id=segment_id, element_name=element_name, element_type=element_type,
                        module_id=module_id, source_fingerprint=self._compute_hash(healed_code),
                        version=1, timestamp=datetime.now(timezone.utc).isoformat(),
                        agent_signature=get_user_signature(), docstring=docstring, start_line=start_line,
                        end_line=end_line, code=healed_code, formatting=fmt_metrics,
                        tokens=token_metrics, sandbox_test=sandbox_result, evaluation=cognitive_eval
                    )
                ),
                source_file_path=str(source_path.resolve()),
                external_dependencies=external_deps,
                execution_context={'python_version': sys.version, 'platform': sys.platform}
            )
            await self._log_ai_metric(
                action='process_block',
                block_id=segment_id,
                inputs={'node_type': element_type, 'code': healed_code[:100]},
                outputs={'packet': packet.model_dump()},
                reason=reason
            )
            return packet
        except Exception as e:
            logging.error(f"Failed to process block {element_name}: {e}", exc_info=True)
            await self._log_ai_metric(
                action='process_block',
                block_id=f"{module_id}.{element_name}.{idx}",
                inputs={'node_type': type(node).__name__},
                outputs={'error': str(e)},
                reason=f"Processing failed: {e}"
            )
            raise
    async def analyze_file(self, source_path: Path) -> Tuple[Optional[Path], List[CodeBlockPacket]]:
        self.console.print(f"\n[bold blue]>>> Analyzing:[/bold blue] [cyan]{source_path}[/cyan]")
        try:
            source_code = await asyncio.to_thread(source_path.read_text, 'utf-8')
            source_fingerprint = self._compute_hash(source_code)
            tree = await asyncio.to_thread(ast.parse, source_code)
        except Exception as e:
            self.console.print(f"[bold red]Error:[/] Failed to parse source file: {e}")
            await self._log_ai_metric(
                action='file_analysis', block_id=source_path.stem,
                inputs={'file': str(source_path)}, outputs={'error': str(e)},
                reason=f"Parse failed: {e}"
            )
            return None, []
        module_id = source_path.stem
        import_segments = [ast.get_source_segment(source_code, n) for n in tree.body if isinstance(n, (ast.Import, ast.ImportFrom))]
        top_level_nodes = [
            n for n in tree.body if isinstance(n, (ast.FunctionDef, ast.ClassDef, ast.AsyncFunctionDef)) and hasattr(n, 'name')
        ]
        tasks = []
        for idx, node in enumerate(top_level_nodes):
            tasks.append(self._process_block(node, source_code, source_fingerprint, module_id, idx, source_path))
        extracted_packets = await asyncio.gather(*tasks, return_exceptions=True)
        valid_packets = [p for p in extracted_packets if not isinstance(p, Exception)]
        if not valid_packets:
            self.console.print("[yellow]No processable code blocks found in file.[/yellow]")
            return None, []
        fidelity = await self._compute_replication_accuracy(source_code, "\n".join([p.code for p in valid_packets]))
        replicated_blocks_for_file = [await self._generate_dynamic_aimb_header(p) + "\n\n" + p.code for p in valid_packets]
        intelligence_header = JINJA_ENV.from_string(INTELLIGENCE_HEADER_TEMPLATE).render(
            segment_count=len(valid_packets),
            fidelity=fidelity * 100,
            agent_signature=get_user_signature(),
            timestamp=datetime.now(timezone.utc).isoformat()
        )
        final_content = f"{intelligence_header}\n\n\"\"\"Evaluation Replica of: {source_path.name} (Fingerprint: {source_fingerprint})\"\"\"\n\n# === Replicated Imports ===\n{'\n'.join(import_segments)}\n\n# === AIMB-Segmented Code Blocks ===\n\n{'\n\n'.join(replicated_blocks_for_file)}"
        replication_dir = PROJECT_ROOT / "smart_bank" / "replications"
        replication_dir.mkdir(parents=True, exist_ok=True)
        replication_path = replication_dir / f"{source_path.stem}_replicated.py"
        await asyncio.to_thread(replication_path.write_text, final_content, 'utf-8')
        self.console.print(f"[bold green] Replication complete.[/bold green] Replicated file written to [cyan]{replication_path}[/cyan]")
        eval_path = self.eval_dir / f"{module_id}_analysis_complete.txt"
        await asyncio.to_thread(eval_path.write_text, f"Analysis completed with {len(valid_packets)} packets.", 'utf-8')
        persist_tasks = [self.bank_manager.store_block(p) for p in valid_packets]
        for p in valid_packets:
            parent_id = p.is_duplicate_of
            persist_tasks.append(self.lineage.track_derivation(
                segment_id=p.segment_id,
                parent_id=parent_id,
                module_id=module_id,
                element_name=p.element_name,
                notes=f"Replication accuracy: {fidelity:.2f}. Cognitive Score: {p.evaluation.cognitive_score:.2f}"
            ))
        await asyncio.gather(*persist_tasks)
        await self.bank_manager.update_module_fingerprint(module_id, source_fingerprint)
        self.last_run_packets.extend(valid_packets)
        await self._log_ai_metric(
            action='file_analysis_v2', block_id=module_id,
            inputs={'file': str(source_path), 'packet_count': len(valid_packets)},
            outputs={'fidelity': fidelity},
            reason=f"Completed advanced analysis of {source_path} with {len(valid_packets)} blocks extracted."
        )
        self.console.print(f"[bold green] Analysis complete.[/bold green] Analysis marker written to [cyan]{eval_path}[/cyan]")
        return eval_path, valid_packets
    async def main_cli(self):
        """Interactive CLI interface for SmartReplicatorV4 with enhanced rich text menus."""
        self.console.rule("[bold yellow]SmartReplicatorV4 Cognitive Engine Console[/bold yellow]")
        while True:
            def display_menu():
                table = Table(title="Main Menu", show_header=True, header_style="bold magenta")
                table.add_column("Option", style="cyan", justify="center")
                table.add_column("Action", style="white")
                table.add_column("Description", style="dim white")
                table.add_row("1", "Run Full Scan", "Analyze all configured source code paths")
                table.add_row("2", "Run Self-Scan", "Analyze the SmartReplicatorV4 codebase itself")
                table.add_row("3", "View Analysis Report", "Display summary of analyzed code blocks")
                table.add_row("4", "Inspect Block Details", "View detailed analysis for a specific block")
                table.add_row("5", "Run Unit Tests", "Execute unit tests for a specific block")
                table.add_row("6", "View Lineage", "Trace the derivation history of a code block")
                table.add_row("7", "Search Similar Blocks", "Find blocks similar to a given code segment")
                table.add_row("8", "Scan Directory and Root", "Scan the directory of the script and the MASTER_CITADEL root")
                table.add_row("9", "Intelligent Replicate and Build", "Replicate and build modules using AI")
                table.add_row("10", "Exit", "Close the console")
                self.console.print(Panel(table, title="SmartReplicatorV4 Options", border_style="blue", padding=(1, 2)))
            async def run_full_scan():
                with Progress(SpinnerColumn(), TextColumn("[progress.description]{task.description}"), transient=True) as progress:
                    task = progress.add_task("Running full scan...", total=None)
                    files = await self.discover_modules_to_replicate()
                    if not files:
                        self.console.print("[bold red]No files found to analyze.[/bold red]")
                        return
                    for file in files:
                        progress.update(task, description=f"Analyzing {file}")
                        await self.analyze_file(file)
                    progress.update(task, description="Full scan completed")
            async def run_self_scan():
                self_file = Path(__file__)
                with Progress(SpinnerColumn(), TextColumn("[progress.description]{task.description}"), transient=True) as progress:
                    task = progress.add_task(f"Self-scanning {self_file}", total=None)
                    await self.analyze_file(self_file)
                    progress.update(task, description="Self-scan completed")
            async def display_report():
                blocks = await self.bank_manager.get_all_blocks_summary()
                if not blocks:
                    self.console.print("[bold red]No analysis data available.[/bold red]")
                    return
                table = Table(title="Analysis Report", show_header=True, header_style="bold magenta")
                table.add_column("Module", style="cyan")
                table.add_column("Element", style="white")
                table.add_column("Type", style="green")
                table.add_column("Cognitive Score", style="yellow", justify="right")
                for block in blocks:
                    table.add_row(block['module_id'], block['element_name'], block['element_type'].capitalize(), f"{block['cognitive_score']:.2f}")
                self.console.print(Panel(table, title="Code Block Summary", border_style="green", padding=(1, 2)))
            async def inspect_block():
                blocks = await self.bank_manager.get_all_blocks()
                if not blocks:
                    self.console.print("[bold red]No blocks available to inspect.[/bold red]")
                    return
                table = Table(title="Available Blocks", show_header=True, header_style="bold magenta")
                table.add_column("Index", style="cyan", justify="center")
                table.add_column("Block ID", style="white")
                table.add_column("Summary", style="dim white")
                for idx, block in enumerate(blocks, 1):
                    packet = CodeBlockPacket.model_validate(block)
                    table.add_row(str(idx), f"{packet.module_id}.{packet.element_name}", packet.evaluation.summary[:80])
                self.console.print(Panel(table, title="Select a Block to Inspect", border_style="blue", padding=(1, 2)))
                choice = IntPrompt.ask("Enter block index", default=1, show_default=True)
                if choice < 1 or choice > len(blocks):
                    self.console.print("[bold red]Invalid block index.[/bold red]")
                    return
                packet = CodeBlockPacket.model_validate(blocks[choice - 1])
                aimb_header = await self._generate_dynamic_aimb_header(packet)
                self.console.print(Panel(f"{aimb_header}\n\n{packet.code}", title=f"Block Details: {packet.module_id}.{packet.element_name}", border_style="cyan", padding=(1, 2)))
                self.console.print(JSON.from_data(packet.model_dump(), indent=2))
            async def run_block_tests():
                blocks = await self.bank_manager.get_all_blocks()
                if not blocks:
                    self.console.print("[bold red]No blocks available to test.[/bold red]")
                    return
                table = Table(title="Available Blocks", show_header=True, header_style="bold magenta")
                table.add_column("Index", style="cyan", justify="center")
                table.add_column("Block ID", style="white")
                table.add_column("Summary", style="dim white")
                for idx, block in enumerate(blocks, 1):
                    packet = CodeBlockPacket.model_validate(block)
                    table.add_row(str(idx), f"{packet.module_id}.{packet.element_name}", packet.evaluation.summary[:80])
                self.console.print(Panel(table, title="Select a Block to Test", border_style="blue", padding=(1, 2)))
                choice = IntPrompt.ask("Enter block index", default=1, show_default=True)
                if choice < 1 or choice > len(blocks):
                    self.console.print("[bold red]Invalid block index.[/bold red]")
                    return
                packet = CodeBlockPacket.model_validate(blocks[choice - 1])
                block_id = f"{packet.module_id}.{packet.element_name}"
                with Progress(SpinnerColumn(), TextColumn("[progress.description]{task.description}"), transient=True) as progress:
                    task = progress.add_task(f"Running tests for {block_id}", total=None)
                    test_code, _ = generate_unit_test_stub(packet.element_name, packet.code)
                    if not test_code:
                        self.console.print("[bold red]Failed to generate test stub.[/bold red]")
                        return
                    test_result = await self._run_unit_test(test_code, block_id)
                    progress.update(task, description="Tests completed")
                result_table = Table(title="Test Results", show_header=True, header_style="bold magenta")
                result_table.add_column("Test Name", style="cyan")
                result_table.add_column("Status", style="green" if test_result.passed else "red")
                result_table.add_column("Output", style="white")
                result_table.add_column("Duration (ms)", style="yellow", justify="right")
                result_table.add_row(test_result.test_name, "PASS" if test_result.passed else "FAIL", test_result.output or "None", f"{test_result.test_duration_ms:.2f}")
                self.console.print(Panel(result_table, title=f"Test Results for {block_id}", border_style="green" if test_result.passed else "red", padding=(1, 2)))
            async def view_lineage():
                blocks = await self.bank_manager.get_all_blocks()
                if not blocks:
                    self.console.print("[bold red]No blocks available to view lineage.[/bold red]")
                    return
                table = Table(title="Available Blocks", show_header=True, header_style="bold magenta")
                table.add_column("Index", style="cyan", justify="center")
                table.add_column("Block ID", style="white")
                table.add_column("Summary", style="dim white")
                for idx, block in enumerate(blocks, 1):
                    packet = CodeBlockPacket.model_validate(block)
                    table.add_row(str(idx), f"{packet.module_id}.{packet.element_name}", packet.evaluation.summary[:80])
                self.console.print(Panel(table, title="Select a Block to View Lineage", border_style="blue", padding=(1, 2)))
                choice = IntPrompt.ask("Enter block index", default=1, show_default=True)
                if choice < 1 or choice > len(blocks):
                    self.console.print("[bold red]Invalid block index.[/bold red]")
                    return
                packet = CodeBlockPacket.model_validate(blocks[choice - 1])
                block_id = f"{packet.module_id}.{packet.element_name}"
                lineage = await self.lineage.get_lineage(packet.segment_id)
                if not lineage:
                    self.console.print(f"[yellow]No lineage data found for {block_id}.[/yellow]")
                    return
                table = Table(title=f"Lineage for {block_id}", show_header=True, header_style="bold magenta")
                table.add_column("Segment ID", style="cyan")
                table.add_column("Parent ID", style="white")
                table.add_column("Generation", style="green")
                table.add_column("Relation", style="yellow")
                table.add_column("Timestamp", style="dim white")
                table.add_column("Notes", style="dim white")
                for entry in lineage:
                    table.add_row(
                        entry['segment_id'],
                        entry['parent_segment_id'] or "None",
                        str(entry['generation']),
                        entry['relation_type'],
                        entry['timestamp'],
                        entry['notes'][:80]
                    )
                self.console.print(Panel(table, title=f"Lineage History for {block_id}", border_style="cyan", padding=(1, 2)))
                await self._log_ai_metric(
                    action='view_lineage',
                    block_id=block_id,
                    inputs={'segment_id': packet.segment_id},
                    outputs={'lineage_entries': len(lineage)},
                    reason=f"Retrieved lineage for {block_id}"
                )
            async def search_for_similar_blocks():
                blocks = await self.bank_manager.get_all_blocks()
                if not blocks:
                    self.console.print("[bold red]No blocks available to search.[/bold red]")
                    return
                query = Prompt.ask("Enter code snippet or description to search for similar blocks")
                with Progress(SpinnerColumn(), TextColumn("[progress.description]{task.description}"), transient=True) as progress:
                    task = progress.add_task("Searching for similar blocks...", total=None)
                    results = await search_similar_blocks(query, blocks, self.sentence_model)
                    progress.update(task, description="Search completed")
                if not results:
                    self.console.print("[yellow]No similar blocks found.[/yellow]")
                    return
                table = Table(title="Similar Blocks", show_header=True, header_style="bold magenta")
                table.add_column("Rank", style="cyan", justify="center")
                table.add_column("Block ID", style="white")
                table.add_column("Similarity Score", style="yellow", justify="right")
                for rank, (segment_id, score) in enumerate(results[:5], 1):
                    table.add_row(str(rank), segment_id, f"{score:.2f}")
                self.console.print(Panel(table, title="Top Similar Blocks", border_style="green", padding=(1, 2)))
                await self._log_ai_metric(
                    action='search_similar',
                    block_id='search',
                    inputs={'query': query[:100]},
                    outputs={'results': [{'segment_id': sid, 'score': float(score)} for sid, score in results[:5]]},
                    reason=f"Performed semantic search for similar blocks"
                )
            async def scan_dir_and_root():
                current_dir = Path(__file__).parent
                root_dir = Path(r"D:\MASTER_CITADEL")
                with Progress(SpinnerColumn(), TextColumn("[progress.description]{task.description}"), transient=True) as progress:
                    task = progress.add_task("Scanning directory and root...", total=None)
                    files = await self.discover_modules_to_replicate()
                    files.extend(await self.discover_modules_to_replicate(str(root_dir)))
                    if not files:
                        self.console.print("[bold red]No files found to analyze.[/bold red]")
                        return
                    for file in files:
                        progress.update(task, description=f"Analyzing {file}")
                        await self.analyze_file(file)
                    progress.update(task, description="Scan completed")
            async def intelligent_replicate_build():
                self.console.print("[yellow]Intelligent Replicate and Build mode[/yellow]")
                source_dirs_str = Prompt.ask("Enter source directories separated by comma (relative to PROJECT_ROOT)")
                source_dirs = [d.strip() for d in source_dirs_str.split(',')]
                scan_and_replicate(source_dirs, PROJECT_ROOT)
            display_menu()
            choice = Prompt.ask("Select an option", choices=["1", "2", "3", "4", "5", "6", "7", "8", "9", "10"], default="1")
            if choice == "1":
                await run_full_scan()
            elif choice == "2":
                await run_self_scan()
            elif choice == "3":
                await display_report()
            elif choice == "4":
                await inspect_block()
            elif choice == "5":
                await run_block_tests()
            elif choice == "6":
                await view_lineage()
            elif choice == "7":
                await search_for_similar_blocks()
            elif choice == "8":
                await scan_dir_and_root()
            elif choice == "9":
                await intelligent_replicate_build()
            elif choice == "10":
                self.console.print("[bold yellow]Exiting...[/bold yellow]")
                break
            Prompt.ask("\n[dim]Press Enter to continue...[/dim]")
async def main():
    # Configure logging
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
    console = Console()
    # Check for OpenAI API key
    if not os.getenv("OPENAI_API_KEY"):
        console.print("[bold yellow]WARNING:[/bold yellow] OPENAI_API_KEY not set. AI-driven analysis will be limited.")
    # Default configuration
    config = {
        "source_code_paths": [], # Default to empty, let config file or user specify
        "evaluation_modules_dir": PROJECT_ROOT / "smart_bank" / "evaluation_modules",
        "db_path": PROJECT_ROOT / "smart_bank" / "data" / "replicator.db",
        "log_level": "INFO"
    }
    # Load configuration from file if available
    config_file = PROJECT_ROOT / "smart_bank" / "replicator_config.json"
    if config_file.exists():
        try:
            with config_file.open('r', encoding='utf-8') as f:
                loaded_config = json.load(f)
                # Ensure paths are loaded as Path objects
                if "source_code_paths" in loaded_config:
                    loaded_config["source_code_paths"] = [Path(p) for p in loaded_config["source_code_paths"]]
                if "evaluation_modules_dir" in loaded_config:
                    loaded_config["evaluation_modules_dir"] = Path(loaded_config["evaluation_modules_dir"])
                if "db_path" in loaded_config:
                    loaded_config["db_path"] = Path(loaded_config["db_path"])
                config.update(loaded_config)
            console.print("[green]Loaded configuration from replicator_config.json[/green]")
        except Exception as e:
            console.print(f"[bold red]ERROR:[/bold red] Failed to load configuration: {e}")
            sys.exit(1)
    # Ensure source_code_paths contains the current script's directory for self-scan
    script_dir = Path(__file__).parent
    if script_dir not in config["source_code_paths"]:
        config["source_code_paths"].append(script_dir)
    # Initialize persistence layers
    try:
        # Ensure the db path directory exists
        db_path = config["db_path"]
        db_path.parent.mkdir(parents=True, exist_ok=True)
        ledger = LedgerEngine(db_path)
        lineage = LineageManager(db_path)
        bank_manager = SmartBankManager(db_path, ledger, lineage)
        await ledger._migrate_schema()
        await lineage._migrate_schema()
        await bank_manager._migrate_schema()
    except Exception as e:
        console.print(f"[bold red]ERROR:[/bold red] Failed to initialize persistence layers: {e}")
        sys.exit(1)
    # Initialize SmartReplicatorV4
    try:
        replicator = SmartReplicatorV4(config, bank_manager)
    except Exception as e:
        console.print(f"[bold red]ERROR:[/bold red] Failed to initialize SmartReplicatorV4: {e}")
        sys.exit(1)
    console.print("[bold blue]Starting SmartReplicatorV4 CLI...[/bold blue]")
    await replicator.main_cli()
if __name__ == "__main__":
    asyncio.run(main())