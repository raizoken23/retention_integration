[project]
name = "power-attention"
version = "1.0.0"
description = "Kernels for symmetric-power-based linear transformers"
authors = [
  { name = "Sean Zhang", email = "sean@manifest.com" },
  { name = "Carles Gelada", email = "cgel@manifest.com" },
  { name = "Jacob Buckman", email = "jacob@manifest.com" },
  { name = "Txus Bach", email = "txus@manifest.com" }
]
classifiers = [
  "Programming Language :: Python :: 3",
  "License :: OSI Approved :: MIT License",
]
readme = "README.md"
requires-python = ">=3.11"
dependencies = [
  'numpy>=2.2',
  "torch==2.6.0",
  "triton>=3.2",
  'setuptools',
  "einops",
  "vidrial==0.1.1",
]

# this is a trick to avoid re-compiling the power attention kernel
# on every install, unless the csrc/power_attention directory (or deps) have changed
# if the python code changes it doesn't matter, since all the other projects
# depend on the power-attention package in editable mode (= python changes are
# transparently visible to consumers)
[tool.uv]
cache-keys = [{ file = "csrc/power_attention/**/*" }, { file = "pyproject.toml" }, { file = "setup.py"}]

[build-system]
requires = [
    "setuptools==69.5.1",
    "torch==2.6.0",
    "triton>=3.2",
    "numpy>=2.2",
    "ninja==1.11.1.3",
    "pybind11==2.13.6",
    "psutil",
    "build",
    "wheel",
]
build-backend = "setuptools.build_meta"

# TODO (sean): add flash_attn and flash-linear-attention
[project.optional-dependencies]
dev = [
  "twine",
  "pytest>=8.3.4",
  "pytest-xdist",
  "cibuildwheel",
  "auditwheel",
  "wheel",
  "build",
  "tiktoken",
  "datasets",
  "httpx",
  "tqdm", 
  "pytest>=8.3.4",
  "pytest-xdist",
  "wheel",
  "build",
  "cibuildwheel",
  "auditwheel",
  "twine",
  "click",
  "pyyaml",
  "bokeh",
  "matplotlib",
  "pymdown-extensions",
  "mkdocs",
  "mkdocstrings-python",
  "requests",
  "wandb",
  "zarr",
  "psutil",
  "bokeh",
  "click",
  "tabulate",
  "flash-linear-attention@git+https://github.com/fla-org/flash-linear-attention",
  "flash_attn==2.7.3",
  "pytest-randomly",
  "pytest-rerunfailures"
]

[tool.cibuildwheel]
# Only build for Linux (since CUDA is required)
build = ["cp311-manylinux*", "cp312-manylinux*"]
skip = ["*-win*", "*-macos*"]
archs = ["x86_64"]
build-frontend = "build"
build-verbosity = 1

before-all = [
    "yum install -y yum-utils",
    "yum-config-manager --add-repo https://developer.download.nvidia.com/compute/cuda/repos/rhel7/x86_64/cuda-rhel7.repo",
    "yum clean all",
    "yum -y install cuda-toolkit-12-4",
    "ln -s /usr/local/cuda-12.4 /usr/local/cuda"
]

manylinux-x86_64-image = "manylinux2014"

[tool.cibuildwheel.linux]
repair-wheel-command = "auditwheel repair -w {dest_dir} {wheel} --plat manylinux2014_x86_64 --exclude libtorch_cpu.so --exclude libtorch_python.so --exclude libc10.so --exclude libtorch_cuda.so --exclude libshm.so --exclude libtorch_global_deps.so --exclude libc10_cuda.so --exclude libcaffe2_nvrtc.so --exclude libtorch_python.so --exclude libtorch_cpu.so --exclude libtorch_cuda.so --exclude libc10.so --exclude libtorch.so --exclude libtorch_cuda_linalg.so"
